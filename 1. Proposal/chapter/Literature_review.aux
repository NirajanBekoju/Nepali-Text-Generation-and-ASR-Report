\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{10}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}A Neural Probabilistic Language Model}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Introduction}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}NPLM Architecture}{10}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Direct Architecture of Probabilistic Language Model}}{11}{}\protected@file@percent }
\newlabel{fig:Probabilistic Language Model Architecture}{{2.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Conclusion and Results}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Attention is all you need}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Introduction}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Transformer Model Architecture}{12}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The Transformer - model architecture}}{12}{}\protected@file@percent }
\newlabel{fig:Transformer Model Architecture}{{2.2}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Encoder and Decoder Stacks}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attention}{13}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Scaled Dot-Product Attention}}{13}{}\protected@file@percent }
\newlabel{fig:Scaled Dot-Product Attention}{{2.3}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Multi-Head Attention consists of attention layer running in parallel}}{14}{}\protected@file@percent }
\newlabel{fig:Multi-Head Attention}{{2.4}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Position wise Feed Forward Neural Network}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Embedding and Softmax}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Positional Encoding}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Conclusion and Results}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Nepali Spelling Correction}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Nepali Speech Recognition Using CNN, GRU, and CTC}{16}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Architecture of proposed ASR system}}{16}{}\protected@file@percent }
\newlabel{fig:Architecture of proposed ASR system}{{2.5}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Summary of experiments with the results}}{17}{}\protected@file@percent }
\newlabel{fig:Summary of experiments with the results}{{2.6}{17}}
\@setckpt{chapter/Literature_review}{
\setcounter{page}{18}
\setcounter{equation}{5}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
}
