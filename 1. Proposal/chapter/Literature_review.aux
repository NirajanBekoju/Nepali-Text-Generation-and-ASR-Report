\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{9}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}A Neural Probabilistic Language Model}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Introduction}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}NPLM Architecture}{9}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Direct Architecture of Probabilistic Language Model}}{10}{}\protected@file@percent }
\newlabel{fig:Probabilistic Language Model Architecture}{{2.1}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Conclusion and Results}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Attention is all you need}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Introduction}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Transformer Model Architecture}{11}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The Transformer - model architecture}}{11}{}\protected@file@percent }
\newlabel{fig:Transformer Model Architecture}{{2.2}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Encoder and Decoder Stacks}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attention}{12}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Scaled Dot-Product Attention}}{12}{}\protected@file@percent }
\newlabel{fig:Scaled Dot-Product Attention}{{2.3}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Multi-Head Attention consists of attention layer running in parallel}}{13}{}\protected@file@percent }
\newlabel{fig:Multi-Head Attention}{{2.4}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Position wise Feed Forward Neural Network}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Embedding and Softmax}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Positional Encoding}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Conclusion and Results}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Nepali Speech Recognition Using CNN, GRU, and CTC}{15}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Architecture of proposed ASR system}}{15}{}\protected@file@percent }
\newlabel{fig:Architecture of proposed ASR system}{{2.5}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Summary of experiments with the results}}{15}{}\protected@file@percent }
\newlabel{fig:Summary of experiments with the results}{{2.6}{15}}
\@setckpt{chapter/Literature_review}{
\setcounter{page}{16}
\setcounter{equation}{5}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
}
