\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Project Management}}{8}{}%
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Discord}}}{8}{}%
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Trello}}}{8}{}%
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Agile Methodology}}}{8}{}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Nepali Text Generation}}{9}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Direct Architecture of Probabilistic Language Model}}{11}{}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces The Transformer - model architecture}}{12}{}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Scaled Dot-Product Attention}}{13}{}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Multi-Head Attention consists of attention layer running in parallel}}{14}{}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Architecture of proposed ASR system}}{16}{}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Summary of experiments with the results}}{17}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Transformer Encoder (src: https://kikaben.com/transformers-encoder-decoder/)}}{22}{}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Transformer Network for ASR \cite {pham2019very}}}{23}{}%
\addvspace {10\p@ }
